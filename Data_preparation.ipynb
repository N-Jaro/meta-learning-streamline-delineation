{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","mount_file_id":"1v0gwWn9e3XfPxIL1wryiDqwHmBdTRaBT","authorship_tag":"ABX9TyPF1fHzkVd/+wUFuZ6ZYQMs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Data Preparation\n","\n"],"metadata":{"id":"zlLDoeK6tkMR"}},{"cell_type":"code","source":["import os\n","import numpy as np\n","import tensorflow as tf\n"],"metadata":{"id":"6fR1ncyVGhps","executionInfo":{"status":"ok","timestamp":1691682218599,"user_tz":300,"elapsed":5342,"user":{"displayName":"Nattapon Jaroenchai","userId":"17092454241854925654"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["\n","class MetaDataLoader:\n","    def __init__(self, data_dir, num_samples_per_location=100):\n","        self.data_dir = data_dir\n","        self.num_samples_per_location = num_samples_per_location\n","\n","    def _load_and_process_data(self, locations):\n","        data_dict = {}\n","\n","        for location in locations:\n","            location_dir = os.path.join(self.data_dir, location)\n","\n","            train_data_path = os.path.join(location_dir, \"train_data.npy\")\n","            train_label_path = os.path.join(location_dir, \"train_label.npy\")\n","            vali_data_path = os.path.join(location_dir, \"vali_data.npy\")\n","            vali_label_path = os.path.join(location_dir, \"vali_label.npy\")\n","\n","            # Load training data and labels\n","            train_data = np.load(train_data_path)\n","            train_label = np.load(train_label_path)\n","\n","            # Load validation data and labels\n","            vali_data = np.load(vali_data_path)\n","            vali_label = np.load(vali_label_path)\n","\n","            # Store data and labels in the data_dict with location as key\n","            data_dict[location] = {\n","                'train_data': train_data,\n","                'train_label': train_label,\n","                'vali_data': vali_data,\n","                'vali_label': vali_label\n","            }\n","\n","        return data_dict\n","\n","    def _create_episode(self, locations):\n","\n","        data_dict = self._load_and_process_data(locations)\n","\n","        #--------- Create the support set -------------\n","        selected_data = []\n","        selected_labels = []\n","        for location in locations:\n","            selected_data.extend(data_dict[location]['train_data'][:num_samples_per_location])\n","            selected_labels.extend(data_dict[location]['train_label'][:num_samples_per_location])\n","\n","\n","        data = np.array(selected_data)\n","        labels = np.array(selected_labels)\n","\n","        # Shuffle the data and labels if needed\n","        indices = np.arange(data.shape[0])\n","        np.random.shuffle(indices)\n","        support_set_data = data[indices]\n","        support_set_labels = labels[indices]\n","\n","        #--------- End create the support set -------------\n","\n","        #--------- Create the query set -------------\n","        selected_data = []\n","        selected_labels = []\n","\n","        for location in locations:\n","            selected_data.extend(data_dict[location]['vali_data'][:num_samples_per_location])\n","            selected_labels.extend(data_dict[location]['vali_label'][:num_samples_per_location])\n","\n","        data = np.array(selected_data)\n","        labels = np.array(selected_labels)\n","\n","        # Shuffle the data and labels if needed\n","        indices = np.arange(data.shape[0])\n","        np.random.shuffle(indices)\n","        query_set_data = data[indices]\n","        query_set_labels = labels[indices]\n","\n","        #--------- End create the query set -------------\n","\n","        return support_set_data, support_set_labels, query_set_data, query_set_labels\n","\n","    def create_multi_episodes(self, num_episodes, locations):\n","        episodes = []\n","        for _ in range(num_episodes):\n","            support_set_data, support_set_labels, query_set_data, query_set_labels = self._create_episode(locations)\n","            episode = {\n","                \"support_set_data\": support_set_data,\n","                \"support_set_labels\": support_set_labels,\n","                \"query_set_data\": query_set_data,\n","                \"query_set_labels\": query_set_labels\n","            }\n","            episodes.append(episode)\n","        return episodes\n"],"metadata":{"id":"OFZIpmjftd8T","executionInfo":{"status":"ok","timestamp":1691682218600,"user_tz":300,"elapsed":11,"user":{"displayName":"Nattapon Jaroenchai","userId":"17092454241854925654"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["%cd '/content/drive/MyDrive/Meta_learning_research/Notebooks/'\n","data_dir = './samples/'  # Replace with the path to your directory containing numpy files\n","locations_meta_training = ['Alexander', 'Rowancreek']\n","locations_meta_testing = ['Covington']\n","num_samples_per_location = 100  # Configure the number of samples per location\n","num_episodes = 10  # Number of episodes\n","\n","data_loader = MetaDataLoader(data_dir, num_samples_per_location)\n","\n","# Create multi episodes for meta-training\n","episodes = data_loader.create_multi_episodes(num_episodes, locations_meta_training)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sxaLqOcrjXk3","executionInfo":{"status":"ok","timestamp":1691682532832,"user_tz":300,"elapsed":314242,"user":{"displayName":"Nattapon Jaroenchai","userId":"17092454241854925654"}},"outputId":"6ecf6f31-eed6-41bb-d28b-7ae8c8d0c68f"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Meta_learning_research/Notebooks\n"]}]},{"cell_type":"code","source":["episodes[1]['support_set_data'].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q-0UAGPGGdU4","executionInfo":{"status":"ok","timestamp":1691682658521,"user_tz":300,"elapsed":8,"user":{"displayName":"Nattapon Jaroenchai","userId":"17092454241854925654"}},"outputId":"be790fd6-1ee6-4247-a09b-48035b0b5110"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(200, 224, 224, 8)"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["# MAML Model\n"],"metadata":{"id":"zm17wUQjj1nP"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n","\n","# Define your U-Net architecture\n","def UNet(input_shape):\n","    # ... (U-Net implementation details)\n","    return model\n","\n","class MemoryModule(tf.keras.layers.Layer):\n","    def __init__(self, num_classes, memory_size, feature_dim):\n","        super(MemoryModule, self).__init__()\n","        self.num_classes = num_classes\n","        self.memory_size = memory_size\n","        self.feature_dim = feature_dim\n","\n","        self.memory = self.add_weight(shape=(num_classes, memory_size, feature_dim),\n","                                      initializer='random_normal',\n","                                      trainable=True)\n","\n","    def call(self, query):\n","        query_norm = tf.nn.l2_normalize(query, axis=-1)\n","        memory_norm = tf.nn.l2_normalize(self.memory, axis=-1)\n","        similarity = tf.matmul(tf.expand_dims(query_norm, 1), memory_norm, transpose_b=True)\n","\n","        _, indices = tf.math.top_k(similarity, k=1)\n","\n","        return tf.gather(self.memory, indices)\n","\n","class MAMLModel(tf.keras.Model):\n","    def __init__(self, memory_module):\n","        super(MAMLModel, self).__init__()\n","        self.memory_module = memory_module\n","        self.unet = UNet()  # Instantiate your U-Net architecture here\n","\n","        self.fc = Dense(num_classes)  # Dense layer for classification\n","\n","    def call(self, x):\n","        features = self.unet(x)  # Pass input through U-Net\n","\n","        query = tf.reshape(features, (features.shape[0], -1))  # Flatten\n","\n","        closest_representations = self.memory_module(query)\n","\n","        combined_features = closest_representations\n","\n","        logits = self.fc(combined_features)\n","        return logits\n","\n","# Hyperparameters\n","num_classes = ...\n","memory_size = ...\n","feature_dim = ...\n","# Define other hyperparameters for U-Net and training\n","\n","# Create memory module\n","memory_module = MemoryModule(num_classes, memory_size, feature_dim)\n","\n","# Create MAML model with memory module and U-Net\n","maml_model = MAMLModel(memory_module)\n","\n","# Loss function and optimizer\n","loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n","\n","# Training loop\n","for task in tasks:  # Iterate over tasks (few-shot tasks)\n","    for task_iteration in range(num_task_iterations):\n","        for example in task.examples:\n","            with tf.GradientTape() as tape:\n","                logits = maml_model(example)\n","                loss = loss_fn(example.labels, logits)\n","\n","            gradients = tape.gradient(loss, maml_model.trainable_variables)\n","\n","            updated_representations = memory_module(query)\n","            updated_representations -= lr_inner * gradients  # Gradient descent update\n","\n","            memory_module.update_memory(query, updated_representations)\n","\n","        with tf.GradientTape() as tape:\n","            meta_loss = compute_meta_loss(maml_model, task)\n","\n","        meta_gradients = tape.gradient(meta_loss, maml_model.trainable_variables)\n","        optimizer.apply_gradients(zip(meta_gradients, maml_model.trainable_variables))\n"],"metadata":{"id":"fyLAqgZFd5bl"},"execution_count":null,"outputs":[]}]}