{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V100","mount_file_id":"1Op4UBvtBhujahKEvvP0t3OnQm6i5bpSg","authorship_tag":"ABX9TyP/4Qah75S/HQ7NLHQGDPxg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fWmMpQjw58ll","executionInfo":{"status":"ok","timestamp":1711504974326,"user_tz":300,"elapsed":89,"user":{"displayName":"Nattapon Jaroenchai","userId":"17092454241854925654"}},"outputId":"1876db97-a4fd-4e31-d64c-e6297c9aee97"},"outputs":[{"output_type":"stream","name":"stdout","text":["Python version:  3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n","TensorFlow version:  2.15.0\n","GPU found at: /device:GPU:0\n"]}],"source":["import tensorflow as tf\n","import tensorflow.keras as keras\n","import tensorflow.keras.backend as keras_backend\n","from data_util import MetaDataLoader\n","from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\n","import random\n","import sys\n","import time\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","print('Python version: ', sys.version)\n","print('TensorFlow version: ', tf.__version__)\n","tf.keras.backend.set_floatx('float64')\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('GPU found at: {}'.format(device_name))"]},{"cell_type":"code","source":["%cd '/content/drive/MyDrive/Meta_learning_research/Notebooks/'\n","import os\n","input_data = './samples/'\n","model_path = './models/'\n","prediction_path = './predicts/'\n","log_path = './logs/'\n","\n","# Create the folder if it does not exist\n","os.makedirs(input_data, exist_ok=True)\n","os.makedirs(model_path, exist_ok=True)\n","os.makedirs(prediction_path, exist_ok=True)\n","\n","# Avaiable backbones for Unet architechture\n","# 'vgg16' 'vgg19' 'resnet18' 'resnet34' 'resnet50' 'resnet101' 'resnet152' 'inceptionv3'\n","# 'inceptionresnetv2' 'densenet121' 'densenet169' 'densenet201' 'seresnet18' 'seresnet34'\n","# 'seresnet50' 'seresnet101' 'seresnet152', and 'attentionUnet'\n","backend = 'unet' # ResNet50 is the best model in the TL study\n","\n","# Added first Convo 8 to 3 channels layers to the random init model\n","name = 'maml-model-' + backend + '-' + str(np.random.randint(1000000))\n","\n","logdir = log_path + name\n","if(os.path.isdir(logdir)):\n","  shutil.rmtree(logdir)\n","os.makedirs(logdir, exist_ok=True)\n","\n","print('model location: '+ model_path+name+'.h5')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OP5MLerAfJN1","executionInfo":{"status":"ok","timestamp":1711502899899,"user_tz":300,"elapsed":1692,"user":{"displayName":"Nattapon Jaroenchai","userId":"17092454241854925654"}},"outputId":"b929f428-9a09-441d-a472-ee7eff02a6f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Meta_learning_research/Notebooks\n","model location: ./models/maml-model-unet-761117.h5\n"]}]},{"cell_type":"code","source":["%cd '/content/drive/MyDrive/Meta_learning_research/Notebooks/'\n","data_dir = './samples/'  # Replace with the path to your directory containing numpy files\n","locations_meta_training = ['Alexander', 'Rowancreek']\n","locations_meta_testing = ['Covington']\n","num_samples_per_location = 10  # Configure the number of samples per location\n","num_episodes = 10  # Number of episodes\n","\n","data_loader = MetaDataLoader(data_dir, num_samples_per_location)\n","\n","# Create multi episodes for meta-training\n","mate_train_episodes = data_loader.create_multi_episodes(num_episodes, locations_meta_training)\n","mate_test_episodes = data_loader.create_multi_episodes(num_episodes, locations_meta_testing)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GD8d7A0CfV8_","executionInfo":{"status":"ok","timestamp":1711503660773,"user_tz":300,"elapsed":666263,"user":{"displayName":"Nattapon Jaroenchai","userId":"17092454241854925654"}},"outputId":"9b7b09ab-1e38-467d-f150-5924488b1779"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Meta_learning_research/Notebooks\n"]}]},{"cell_type":"code","source":["len(mate_train_episodes) #[0][\"support_set_data\"].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yCURp0oLkqeX","executionInfo":{"status":"ok","timestamp":1711503833537,"user_tz":300,"elapsed":74,"user":{"displayName":"Nattapon Jaroenchai","userId":"17092454241854925654"}},"outputId":"f844bc1d-2f95-4a81-ea43-4cd2c5d5c3f6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["class UnetModel(tf.keras.Model):\n","    def __init__(self, num_classes):\n","        super(UnetModel, self).__init__()\n","        # Encoder: Downsampling\n","        self.conv1 = Conv2D(64, 3, activation='relu', padding='same')\n","        self.conv2 = Conv2D(64, 3, activation='relu', padding='same')\n","        self.pool1 = MaxPooling2D(pool_size=(2, 2))\n","\n","        self.conv3 = Conv2D(128, 3, activation='relu', padding='same')\n","        self.conv4 = Conv2D(128, 3, activation='relu', padding='same')\n","        self.pool2 = MaxPooling2D(pool_size=(2, 2))\n","\n","        # Bottleneck\n","        self.conv5 = Conv2D(256, 3, activation='relu', padding='same')\n","        self.conv6 = Conv2D(256, 3, activation='relu', padding='same')\n","\n","        # Decoder: Upsampling\n","        self.upconv1 = Conv2DTranspose(128, 2, strides=(2, 2), padding='same')\n","        self.conv7 = Conv2D(128, 3, activation='relu', padding='same')\n","        self.conv8 = Conv2D(128, 3, activation='relu', padding='same')\n","\n","        self.upconv2 = Conv2DTranspose(64, 2, strides=(2, 2), padding='same')\n","        self.conv9 = Conv2D(64, 3, activation='relu', padding='same')\n","        self.conv10 = Conv2D(64, 3, activation='relu', padding='same')\n","\n","        # Output Layer\n","        self.conv11 = Conv2D(num_classes, 1, activation='softmax')\n","\n","    def forward(self, inputs):\n","        # Encoder\n","        c1 = self.conv1(inputs)\n","        c2 = self.conv2(c1)\n","        p1 = self.pool1(c2)\n","\n","        c3 = self.conv3(p1)\n","        c4 = self.conv4(c3)\n","        p2 = self.pool2(c4)\n","\n","        # Bottleneck\n","        c5 = self.conv5(p2)\n","        c6 = self.conv6(c5)\n","\n","        # Decoder\n","        u1 = self.upconv1(c6)\n","        u1 = tf.concat([u1, c4], axis=-1)  # Skip connection\n","        c7 = self.conv7(u1)\n","        c8 = self.conv8(c7)\n","\n","        u2 = self.upconv2(c8)\n","        u2 = tf.concat([u2, c2], axis=-1)  # Skip connection\n","        c9 = self.conv9(u2)\n","        c10 = self.conv10(c9)\n","\n","        outputs = self.conv11(c10)\n","        return outputs"],"metadata":{"id":"HKfLX7VkVIZV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def loss_function(pred_y, y):\n","    return keras_backend.mean(keras.losses.mean_squared_error(y, pred_y))\n","\n","def np_to_tensor(list_of_numpy_objs):\n","    return (tf.convert_to_tensor(obj) for obj in list_of_numpy_objs)\n","\n","\n","def compute_loss(model, x, y, loss_fn=loss_function):\n","    logits = model.forward(x)\n","    mse = loss_fn(y, logits)\n","    return mse, logits\n","\n","def train_batch(x, y, model, optimizer):\n","    tensor_x, tensor_y = np_to_tensor((x, y))\n","    with tf.GradientTape() as tape:\n","        loss, _ = compute_loss(model, x, y)\n","    gradients = tape.gradient(loss, model.trainable_variables)\n","    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","    return loss"],"metadata":{"id":"H3g9Qi3RJ2qC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def copy_model(model, x=None):\n","    '''Copy model weights to a new model.\n","\n","    Args:\n","        model: model to be copied.\n","        x: An input example. This is used to run\n","            a forward pass in order to add the weights of the graph\n","            as variables.\n","    Returns:\n","        A copy of the model.\n","    '''\n","\n","    copied_model = SineModel()\n","\n","    # If we don't run this step the weights are not \"initialized\"\n","    # and the gradients will not be computed.\n","    copied_model.forward(tf.convert_to_tensor(x))\n","\n","    copied_model.set_weights(model.get_weights())\n","    return copied_model\n"],"metadata":{"id":"bpUsQIK5J41L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Regular training"],"metadata":{"id":"AIKo5Q2NKJFB"}},{"cell_type":"code","source":["def train_reg(epochs, x, y, dataset,mdl=None, lr=0.001, log_steps=100):\n","    if mdl is not None :\n","        model = mdl\n","    else:\n","        model = UnetModel()\n","    optimizer = keras.optimizers.Adam(learning_rate = lr)\n","    losses = []\n","    for epoch in range(epochs):\n","        if log_steps is not None:\n","            print(\"====== Epoch : \" +str(epoch)+ \" ====== \")\n","\n","        total_loss = 0\n","        curr_loss = 0\n","        tmp = 0\n","        for i, sinusoid_generator in enumerate(dataset):\n","            # x, y = sinusoid_generator.batch()\n","            loss = train_batch(x, y, model, optimizer)\n","            total_loss += loss\n","            curr_loss = total_loss / (i + 1.0)\n","\n","            tmp = i\n","            if log_steps is not None:\n","                if i % log_steps == 0 and i > 0:\n","                    print('Step {}: loss = {}'.format(i, curr_loss))\n","        losses.append(curr_loss)\n","    plt.plot(losses)\n","    plt.xlabel(\"Adaptation steps\")\n","    plt.title(\"Mean Absolute Error Performance (Normal)\")\n","    plt.ylabel(\"Loss\")\n","    plt.show()\n","    return model, np.array(losses)"],"metadata":{"id":"3mWNq75UJ645"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nn = train_reg(5, train_ds)"],"metadata":{"id":"3c9Mm_EQJ8zO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Reptile Training"],"metadata":{"id":"vZiiG-whKMX6"}},{"cell_type":"code","source":["def train_reptile( epochs, dataset, mdl=None, lr_inner=0.001, lr_outer=0.01, batch_size=1, log_steps=100, k=1):\n","    #Step 1 : initialize model\n","    if mdl is not None :\n","        model = mdl\n","    else:\n","        model = SineModel()\n","    inner_optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=lr_inner)\n","    outer_optimizer = keras.optimizers.Adam(learning_rate=lr_outer)\n","    losses = []\n","\n","    # Step 2 : iteration\n","    for epoch in range(epochs):\n","        if log_steps is not None:\n","            print(\"====== Epoch : \" +str(epoch)+ \" ====== \")\n","        total_loss = 0\n","\n","        start = time.time()\n","        #Step 3 & 4 : get sample task from dataset\n","        for i, t in enumerate(random.sample(dataset, len(dataset))):\n","            x, y = np_to_tensor(t.batch())\n","            model.forward(x)\n","\n","            # save current parameter\n","            old_weights = model.get_weights()\n","\n","            model_copy = copy_model(model, x)\n","            # Step 5 : Compute W with SGD\n","            for _ in range(k):\n","                loss = train_batch(x, y, model_copy, inner_optimizer)\n","\n","            # Step 6 : update model parameter\n","            after_weights = model_copy.get_weights()\n","            step_size = lr_inner * (1 - epoch / epochs) # linear scheduling method\n","            new_weights = [ old_weights[i] + ((old_weights[i] - after_weights[i]) * step_size)\n","                           for i in range(len(model.weights))]\n","            model.set_weights(new_weights)\n","\n","            # additional step for outer optimization\n","            if (i+1) % batch_size == 0:\n","                test_loss = train_batch(x, y, model, outer_optimizer)\n","            else:\n","                test_loss, logits = compute_loss(model, x, y)\n","\n","            # Logs\n","            total_loss += test_loss\n","            loss = total_loss / (i+1.0)\n","\n","            if log_steps is not None:\n","                if i % log_steps == 0 and i > 0:\n","                    print('Step {}: loss = {}, Time to run {} steps = {}'.format(i, loss, log_steps, time.time() - start))\n","                    start = time.time()\n","\n","        losses.append(loss)\n","    plt.plot(losses)\n","    plt.xlabel(\"Adaptation steps\")\n","    plt.title(\"Mean Absolute Error Performance (REPTILE)\")\n","    plt.ylabel(\"Loss\")\n","    plt.show()\n","    return model, np.array(losses)"],"metadata":{"id":"Xszvyx1hJ_yZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["reptile = train_reptile(5, train_ds)"],"metadata":{"id":"YZzLSnqSKCKy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Test\n","\n","This test are comparing regular model vs reptile model with discrepancy in data compared to initial training. We would like to know how reptile perform while discrepancy getting bigger. The model will be trained in 100 task per discrepancy setup in 10 epochs each for 3 times and we will calculate the average of loss for better comparison variable."],"metadata":{"id":"unJXQjgvKGoC"}},{"cell_type":"code","source":["x_copy, _ = train_ds[0].batch()\n","def test_session(model, train_func):\n","    all_losses = []\n","    p = [0.3 , 0.5, 0.7, 0.9, 1]\n","    for i in range (0,5):\n","        title = \"Amplitude : \"+ str((i+2)) + \" Phase : \"+ str(p[i])+\"*phi)\"\n","        print(title)\n","        last_losses = 0\n","        all_res = []\n","        for j in range(0, 5):\n","            print(\"======= \", j+1, \" run ========\")\n","            test_task = [SinusoidGenerator(K=100, amplitude = i+2 , phase=p[i])]\n","            cp_model = copy_model(model, x_copy)\n","            res = train_func(epochs=100, mdl=cp_model, dataset=test_task, log_steps=None)\n","            all_res.append(res[1])\n","            last_losses+=res[1][-1]\n","        plt.plot(all_res[0])\n","        plt.plot(all_res[1])\n","        plt.plot(all_res[2])\n","        plt.plot(all_res[3])\n","        plt.plot(all_res[4])\n","        plt.title(title)\n","        plt.legend([\"Run 1\", \"Run 2\", \"Run 3\", \"Run 4\", \"Run 5\"], loc=(1.05, 0.5))\n","        plt.xlabel(\"Adaptation steps\")\n","        plt.ylabel(\"Loss\")\n","        plt.show()\n","        all_losses.append(last_losses/5)\n","\n","    category = [\"Test_Ds1\", \"Test_Ds2\", \"Test_Ds3\",\"Test_Ds4\",\"Test_Ds5\"]\n","    fig, ax = plt.subplots()\n","    ax.plot(category, all_losses, label=\"loss\")\n","    ax.legend(loc=(1.05, 0.5))\n","    plt.show()\n","    return all_losses"],"metadata":{"id":"LlNQ0s7LKGIP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["reptile_res = test_session(reptile[0], train_reptile)"],"metadata":{"id":"Nw_ZatqsKe8I"},"execution_count":null,"outputs":[]}]}